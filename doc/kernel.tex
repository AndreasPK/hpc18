\documentclass[]{article}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}

%opening
\title{}
\author{}


\lstset{
	basicstyle=\small,
	language=C
}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Roofline model}

\subsection{Specifications}
\begin{itemize}
    \item \texttt{mars}:
        \begin{itemize}
            \item Intel Xeon E7-8850 @ 2.00 GHz
            \item 80 Cores
        \end{itemize}
        This processor is equipped with several 128-bit (XMMxx) registers due to SSE2 and a separate floating point adder and multiplier that operate on these registers. Since in each cycle 4 single-precision floating-point numbers can be added and 4 more can be multiplied, each core is capable of 8 single-precision floating-point operations per cycle. \cite{agnerorg}
        \begin{itemize}
            \item Theoretical peak performance: \\
            $1 \, \text{instruction} \cdot 8 \, \text{operations} \cdot 2.00 \, \text{GHz} \cdot 80 \, \text{cores} = \bold{1280.00} \, \textbf{GFLOPS/s}$
            
            \item Empirical peak memory bandwidth: $\bold{30.5} \, \textbf{GB/s}$ \\
        \end{itemize}
    \item \texttt{earth}:
        \begin{itemize}
            \item Intel Core i5 750 @ 2.67 GHz
            \item 4 Cores
        \end{itemize}
        This processor also uses the SSE2 extensio but has only one unit for floating-point addition and multiplication. Therefore, at most 4 floating-point calculations are executed with each cycle.
        \begin{itemize}
            \item Theoretical peak performance: \\
            $1 \, \text{instruction} \cdot 4 \, \text{operations} \cdot 2.67 \, \text{GHz} \cdot 4 \, \text{cores} = \bold{42.72 \, \textbf{GFLOPS/s}}$
            
            \item Peak memory bandwidth: $\bold{12.2} \, \textbf{GB/s}$ \\
        \end{itemize}
\end{itemize}

\subsection{Parameters \& Benchmark output}
\begin{itemize}
    \item Parameters used to compile \texttt{numa-stream}: \\
    \texttt{
        gcc -O3 -std=c99 stream.c -lnuma -fopenmp \textbackslash \\
        -DN=80000000 -DNTIMES=100 -o stream-gcc
    }
    
    \item Output of \texttt{stream-gcc} on \texttt{earth}: \\
    \texttt{\tiny 
------------------------------------------------------------- \\
STREAM version $Revision: 5.9 $ \\
------------------------------------------------------------- \\
This system uses 8 bytes per DOUBLE PRECISION word. \\
------------------------------------------------------------- \\
Array size = 80000000 \\
Total memory required = 1831.1 MB. \\
Each test is run 100 times, but only \\
the *best* time for each is used. \\
------------------------------------------------------------- \\
Number of Threads requested = 2 \\
Number of available nodes = 1 \\
------------------------------------------------------------- \\
Your clock granularity/precision appears to be 1 microseconds. \\
Each test below will take on the order of 81670 microseconds. \\
   (= 81670 clock ticks) \\
Increase the size of the arrays if this shows that \\
you are not getting at least 20 clock ticks per test. \\
------------------------------------------------------------- \\
WARNING -- The above is only a rough guideline. \\
For best results, please be sure you know the \\
precision of your system timer. \\
------------------------------------------------------------- \\
Function      Rate (MB/s)   Avg time     Min time     Max time \\
Copy:       11315.5286       0.1137       0.1131       0.1237 \\
Scale:      11134.7738       0.1156       0.1150       0.1184 \\
Add:        12139.7541       0.1589       0.1582       0.1625 \\
Triad:      12197.3056       0.1581       0.1574       0.1597 \\
------------------------------------------------------------- \\
Solution Validates \\
-------------------------------------------------------------
    }
    
    \newpage
    
    \item Output of \texttt{stream-gcc} on \texttt{mars}: \\
    \texttt{\tiny 
------------------------------------------------------------- \\
STREAM version $Revision: 5.9 $ \\
------------------------------------------------------------- \\
This system uses 8 bytes per DOUBLE PRECISION word. \\
------------------------------------------------------------- \\
Array size = 80000000 \\
Total memory required = 1831.1 MB. \\
Each test is run 100 times, but only \\
the *best* time for each is used. \\
------------------------------------------------------------- \\
Number of Threads requested = 80 \\
Number of available nodes = 8 \\
------------------------------------------------------------- \\
Your clock granularity/precision appears to be 1 microseconds. \\
Each test below will take on the order of 20711 microseconds. \\
   (= 20711 clock ticks) \\
Increase the size of the arrays if this shows that \\
you are not getting at least 20 clock ticks per test. \\
------------------------------------------------------------- \\
WARNING -- The above is only a rough guideline. \\
For best results, please be sure you know the \\
precision of your system timer. \\
------------------------------------------------------------- \\
Function      Rate (MB/s)   Avg time     Min time     Max time \\
Copy:       30344.0333       0.0429       0.0422       0.0441 \\
Scale:      30521.8913       0.0424       0.0419       0.0440 \\
Add:        30126.6472       0.0645       0.0637       0.0656 \\
Triad:      30260.5691       0.0643       0.0634       0.0660 \\
------------------------------------------------------------- \\
Solution Validates \\
------------------------------------------------------------- \\
    }
\end{itemize}

\newpage

\subsection{Roofline plot} 
\includegraphics[scale=0.4, angle=90]{roofline-earth-mars-pure.png}

\newpage

\section{Maximum performance of the kmeans kernel}

The kernel of the \texttt{kmeans} algoirthm is in the \texttt{kmeans\_clustering} function.

The function is split into three main parts:
\begin{itemize}
	\item Initialization (happens once)
	\item In the main loop:
	\begin{itemize}
		\item Calculating membership to existing clusters for each point.
		\item Update clusters.
	\end{itemize}
\end{itemize}

We can safely ignore initialization for our performance consideration which leaves the main loop.

\newpage
\subsection{The Main Loop}

We can split the main loop again into three somewhat distinct parts.
\begin{itemize}
	\item In a parallel loop:
\begin{itemize}
	\item Find the nearest cluster for each point.
	\item Collect partial updates on the position of clusters.
\end{itemize}
	\item Reduce partial information and update positions of clusters 	
\end{itemize}

For our analysis we assume 5 clusters and 34 features which matches
the default of kmeans and our input datasets.

The only significant contributors here are the loops to find the nearest cluster and updating the cluster information.
For this reason we will look only at these inner loops and not at the rest of the main loop.


\subsubsection{Main loop:Finding the nearest cluster}
\begin{lstlisting}[caption={Inlined representation of find\_nearest\_point}]
for (i=0; i<nclusters; i++) {
	float dist=0;
	for (j=0; j<nfeatures; j++)
		dist += (pt1[j]-clusters[i][j]) * (pt1[j]-clusters[i][j]);
	if (dist < min_dist) {
		min_dist = dist;
		index    = i;
	}
}
\end{lstlisting}
The kernel here is: \texttt{dist += (pt1[j]-pt2[j]) * (pt1[j]-pt2[j])}.\newline

We get 8
\footnote{Actually 12. But we can ignore the store of dist as it will be held in a register.}
byte of memory bandwith and three floating point operations.
Memory bandwith consists of 2 loads for each iteration.
Operations are two additions and one multiplication.
\footnote{Treating subtraction as addition.}
\footnote{While there are two subtractions in the code gcc will eliminate one through common subexpression elimination.}
\begin{itemize}[noitemsep,nolistsep]
	\item Read: \texttt{pt1[j]}
	\item Read: \texttt{clusters[i][j]}
\end{itemize}

Immediatly obvious from the snippet above we get at least $nclusters * nfeatures$ iterations.
With one call to \texttt{find\_nearest\_point} for each point per iteration of the main loop this gives us:\\
$$iterations = n * nclusters * nfeatures = n * 170$$

Even for our smallest data sample which has 100 points this already gives us $17000$ iterations and will dominate the computational demands for any reasonably large size of n.

If we stop here our Arithmethic Intensity is 3 FP operations per 8 byte. Giving an AI of $\frac{0,375 Ops}{Byte}$
\footnote{Although in practice caches will usually hold both pt1 and the clusters}


\subsubsection{Main Loop:Updating cluster information.}
\begin{lstlisting}[caption={Updating (partial) cluster information},label=lblUpdPartClst]
for (j=0; j<nfeatures; j++)
	partial_new_centers[tid][index][j] += feature[i][j];
\end{lstlisting}
This happens again for each point in the dataset for each iteration of the Main loop.
This gives us for iterations:
$$iterations = n * nfeatures = 34$$

While this is not as big an contributor as the loop finding the nearest cluster
it still contributes a noteworthy amount of computational demands.

In particular for each iteration here we have two loads (8 Byte) and one addition.
\footnote{Although it is fair to assume that partial\_new\_centers[tid][index][j] will be cached.}


\subsubsection{Main Loop:Reduction loop one}
\begin{lstlisting}[caption={Reduction pt1}]
/* let the main thread perform the array reduction */
for (i=0; i<nclusters; i++) {
	for (j=0; j<nthreads; j++) {
		new_centers_len[i] += partial_new_centers_len[j][i];
		partial_new_centers_len[j][i] = 0.0;
		for (k=0; k<nfeatures; k++) {
			new_centers[i][k] += partial_new_centers[j][i][k];
			partial_new_centers[j][i][k] = 0.0;
		}
	}
}
\end{lstlisting}

The amount of iterations is static in regards to the number of items we work on.
It is given by $ iterations = clusters * threads * features$.
In our case when using all cares we get $ iterations = 5 * 80  * 34 = 13600$.
Which however is neglible for nontrivial datasets so we don't include it in our arithmetic intensity computation. 

%Ignoring integer operations and caching completely we only have to consider the innermost loop for AI.
%
%This gives for memory bandwith 16 Bytes:
%\begin{itemize}[noitemsep,nolistsep]
%	\item Read \texttt{new\_centers}
%	\item Read \texttt{partial\_new\_centers}
%	\item Write \texttt{new\_centers}
%	\item Write \texttt{partial\_new\_centers}
%\end{itemize}
%
%For fp operations we get a single addition from:\\
%\texttt{new\_centers[i][k] += partial\_new\_centers[j][i][k]}

\subsubsection{Main Loop:Reduction loop two}
\begin{lstlisting}[caption={Reduction pt2}]
/* replace old cluster centers with new_centers */
for (i=0; i<nclusters; i++) {
	for (j=0; j<nfeatures; j++) {
		if (new_centers_len[i] > 0)
			clusters[i][j] = new_centers[i][j] / new_centers_len[i];
		new_centers[i][j] = 0.0;   /* set back to 0 */
	}
	new_centers_len[i] = 0;   /* set back to 0 */
}
\end{lstlisting}
With at most $clusters * features = 5 * 34 = 170 = iterations$ this loop has no significant impact on overalls performance.
For this reason we will ignore it.

\subsection{kmeans: Roofline Model}

Table \ref{tab:AI} gives us the AI for the inner main loop based on it's two essential parts according to the roofline model.

It is however important to point out this does assume NO caching. In practice most of the memory pressure will be absorbed by the cache.

According to the roofline model this gives us a demand of $$\frac{1280GFlops}{0,31Flop/Byte} = 4129 Gb/s$$ on the memory system for 80 Threads.

\begin{table}[ht]
	\centering
	\caption{Arithmethic Intensity according to the Roofline Model}
	\label{tab:AI}
	\begin{tabular}{|r|r|r|r|}
		\hline
		& Find nearest Cluster & Update cluster information & Total \\ \hline
		Iterations           & $n * 170$            & $n*34$                     & -     \\ \hline
		Memory (Byte)              & $8*170$              & $12*34$                      & 1768  \\ \hline
		Operations (Flop)          & $3*170$              & $1*34$                       & 544   \\ \hline
		Arithmetic Intensity (Flop/Byte) & 0,375                & 0,08                       & 0,31  \\ \hline
	\end{tabular}
\end{table}

\subsection{kmeans: Maximum Performance}

When considering the maximum performance of the given code there are multiple things to consider:

\begin{itemize}
	\item The kernel does not use SIMD instructions. This immediatly brings down maximum performance by a factor of 4!
	\item The kernel is not balanced. There is actually only a single relevant multiplication in the inner loop! Which can happen in parallel to the additions.
\end{itemize}

This means performance is limited by the addition ALU primarily.

In \texttt{find\_nearest\_point} we have two additions and one multiplication per result. This mean on average we will execute an addition every cycle and a multiplication every second.
So we execute 1,5 Instructions per cycle instead of the maximum of 2 in this kernel because of the mul/add imbalance.

We could additionally take into account the smaller loop \autoref{lblUpdPartClst} which is limited purely by addition.
However this would bring down flops/cycle only slightly to 1,41 so for simplicity we will use 1.5 for further analysis.

Maximum FP performance under these considerations is:
$$1280 GFlops * 0.25 \text{(no SIMD)}  * 0.75 \text{(imbalance)} = 240 GFlops$$.

Which gives us for the demand on the memory subsystem for 80 Threads:
$$\frac{240 GFlops}{0,31 Flop/Byte} = 774 GB/s$$

And 10 Threads:
$$\frac{30 GFlops}{0,31 Flop/Byte} = 96 GB/s$$

However since many of the inputs to the kernel can be cached this is not representative of the actualy demands on the memory system as our measurements show.

\end{document}
